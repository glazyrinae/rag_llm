services:
  #docker exec ollama ollama pull tinyllama:1.1b
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ./deploy/ollama_models:/root/.ollama
    restart: unless-stopped
    networks:
      - shared_net

  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    ports:
      - "3000:8080"
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - OLLAMA_API_BASE_URL=http://ollama:11434/api
      - HF_ENDPOINT=https://hf-mirror.com
      - OLLAMA_REQUEST_TIMEOUT=300
      - DOWNLOAD_TIMEOUT=600
      - HF_HUB_DISABLE_SYMLINKS_WARNING=1
    volumes:
      - ./deploy/open-webui-data:/app/backend/data
      - ./deploy/ollama_models:/root/.ollama  # общая папка с моделями
    depends_on:
      - ollama
    restart: unless-stopped
    networks:
      - shared_net

  qdrant:
    image: qdrant/qdrant:latest
    ports:
      - ${QDRANT__SERVICE__HTTP_PORT}:6333 #для web gui
      - "6334:6334" #gRPC (gRPC Remote Procedure Calls)
    volumes:
      - ./deploy/qdrant_data:/qdrant/storage
    environment:
      - QDRANT__SERVICE__HTTP_PORT=${QDRANT__SERVICE__HTTP_PORT}
    networks:
      - shared_net

  # поддержка контекста
  # можно ли разделять поиск например только искать по коду или по тексту
  # можно ли сделать треды для кода для общих вопросов типа чата id
  # использовать другую модель может отдавать более кач ответ (про сервак) qwen2.5-coder:7b !qwen3-coder:30b!
  api:
    build:
      context: ./deploy
    user: "${UID}:${GID}"
    command: >
      sh -c "uvicorn main:app --host 0.0.0.0 --port 8000 --reload"
    volumes:
      - ./deploy/data:/app/data
      - ./app:/app
      - ./deploy/embedding_models:/app/embedding_models
    environment:
      - QDRANT_HOST=qdrant
      #- HF_ENDPOINT=https://hf-mirror.com
      - HF_HUB_DOWNLOAD_TIMEOUT=600  # 10 минут
      - HF_TRANSFER_TIMEOUT=600
      - HF_REQUEST_TIMEOUT=600
      - HF_HUB_ETAG_TIMEOUT=600
      - QDRANT_PORT=6333
      - OPENROUTER_API_KEY=${OPENROUTER_API_KEY}
    env_file:
      - ./.env
    ports:
      - 5005:8000
    networks:
      - shared_net

  redis:
    image: redis:6.2.6
    user: "${UID}:${GID}"
    volumes:
      - ./deploy/redis_data:/data
    networks:
      - shared_net

  queue:
    user: "${UID}:${GID}"
    build:
      context: ./deploy
    env_file:
      - ./.env
    volumes:
      - ./app/:/app
      - ./deploy/embedding_models:/app/embedding_models
    environment:
      - QDRANT_HOST=qdrant
      #- HF_ENDPOINT=https://hf-mirror.com
      - HF_HUB_DOWNLOAD_TIMEOUT=600  # 10 минут
      - HF_TRANSFER_TIMEOUT=600
      - HF_REQUEST_TIMEOUT=600
      - HF_HUB_ETAG_TIMEOUT=600
      - QDRANT_PORT=6333
      - OPENROUTER_API_KEY=${OPENROUTER_API_KEY}
    networks:
      - shared_net
    command: python ./manage.py

  telegram-bot:
    build: ./bot/deploy
    container_name: telegram-bot
    restart: unless-stopped
    volumes:
      - ./bot/:/app
    depends_on:
      - api
    env_file:
      - ./bot/.env
    networks:
      - shared_net

# создал внешнюю сеть для тестирования в контейнерах
networks:
  shared_net:
    external: true
    name: shared_network
