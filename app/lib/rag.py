# app/rag_system.py
import os
from typing import List, Dict, Any
import logging
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferWindowMemory
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Qdrant
from langchain.embeddings import HuggingFaceEmbeddings
#from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate
from qdrant_client import QdrantClient
from qdrant_client.http import models  # –î–æ–±–∞–≤–ª—è–µ–º –∏–º–ø–æ—Ä—Ç
from lib.file_processor import FileProcessorFacade

logger = logging.getLogger(__name__)

class Rag:
    def __init__(
        self,
        embeddings: str = "sentence-transformers/all-mpnet-base-v2",
        cache_dir: str = "/app/embedding_models",
    ):
        os.makedirs(cache_dir, exist_ok=True)
        
        # –¢–∏–ø –º–æ–¥–µ–ª–∏ "local" –∏–ª–∏ "cloud"
        self.llm_type = "local"
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Qdrant –∫–ª–∏–µ–Ω—Ç–∞
        self.qdrant_client = QdrantClient(
            host=os.getenv("QDRANT_HOST", "localhost"),
            port=int(os.getenv("QDRANT_PORT", "6333")),
        )

        # –≠–º–±–µ–¥–¥–∏–Ω–≥–∏
        self.embeddings = HuggingFaceEmbeddings(
            model_name=embeddings,
            cache_folder=cache_dir,
            model_kwargs={"device": "cpu"},
        )

        # –í—ã–Ω–µ—Å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–¥ —Å—é–¥–∞ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π —Å–æ–∑–¥–∞–Ω–∏—è –Ω–∞–∑–≤–∞–Ω–∏—è —Å—Ö–µ–º—ã
        self.vector_store = self._init_vector_store("knowledge base")


        # –°–ø–ª–∏—Ç—Ç–µ—Ä –¥–ª—è –∫–æ–¥–∞
        self.code_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=150,
            separators=["\n\nclass ", "\n\ndef ", "\n\nasync def ", "\n\n# ", "\n\n", "\n", " "],
            length_function=len,
        )

        self.llm = None
        self.qa_chain = None
        self.memory = None

    def _init_vector_store(self, collection_name: str):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –∫–æ–ª–ª–µ–∫—Ü–∏–π
            collections = self.qdrant_client.get_collections().collections
            collection_names = [col.name for col in collections]
            
            if collection_name in collection_names:
                logger.info(f"‚úÖ –ö–æ–ª–ª–µ–∫—Ü–∏—è '{collection_name}' —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
            else:
                # –ï—Å–ª–∏ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –Ω–µ—Ç - —Å–æ–∑–¥–∞–µ–º
                logger.info(f"üÜï –°–æ–∑–¥–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é '{collection_name}'")
                self.qdrant_client.create_collection(
                    collection_name=collection_name,
                    vectors_config=models.VectorParams(
                        size=768,  # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–ª—è all-mpnet-base-v2
                        distance=models.Distance.COSINE
                    )
                )
                logger.info(f"‚úÖ –ö–æ–ª–ª–µ–∫—Ü–∏—è '{collection_name}' —Å–æ–∑–¥–∞–Ω–∞")
                
        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–æ–ª–ª–µ–∫—Ü–∏—é: {e}")
            # –ü—ã—Ç–∞–µ–º—Å—è —Å–æ–∑–¥–∞—Ç—å –∫–æ–ª–ª–µ–∫—Ü–∏—é, –µ—Å–ª–∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ —É–¥–∞–ª–∞—Å—å
            try:
                self.qdrant_client.create_collection(
                    collection_name=collection_name,
                    vectors_config=models.VectorParams(
                        size=768,
                        distance=models.Distance.COSINE
                    )
                )
                logger.info(f"‚úÖ –ö–æ–ª–ª–µ–∫—Ü–∏—è '{collection_name}' —Å–æ–∑–¥–∞–Ω–∞ –ø–æ—Å–ª–µ –æ—à–∏–±–∫–∏ –ø—Ä–æ–≤–µ—Ä–∫–∏")
            except Exception as create_error:
                if "already exists" in str(create_error):
                    logger.info(f"‚úÖ –ö–æ–ª–ª–µ–∫—Ü–∏—è '{collection_name}' —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
                else:
                    raise create_error

        # –°–æ–∑–¥–∞–µ–º –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
        return Qdrant(
            client=self.qdrant_client,
            collection_name=collection_name,
            embeddings=self.embeddings
        )


    def init_llm(
        self, 
        model: str = "Qwen/Qwen3-Coder-30B-A3B-Instruct", 
        llm_type: str = "local",  # "local" –∏–ª–∏ "cloud"
        api_key: str = None, # –î–ª—è –æ–±–ª–∞—á–Ω–æ–π —Ç–æ–ª—å–∫–æ –Ω—É–∂–µ–Ω
        base_url: str = "http://ollama:11434",
    ):
        
        self.llm_type = llm_type
        
        if llm_type == "local":
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º Ollama
            from langchain.llms import Ollama
            self.llm = Ollama(
                model=model,
                base_url=base_url,
                temperature=0.1
            )
            logger.info(f"‚úÖ –õ–æ–∫–∞–ª—å–Ω–∞—è LLM '{model}' –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
            
        elif llm_type == "cloud":
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–±–ª–∞—á–Ω—É—é –º–æ–¥–µ–ª—å (–æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∫–æ–¥)
            if not api_key:
                raise ValueError("API key required for cloud model")
                
            from langchain.chat_models import ChatOpenAI
            self.llm = ChatOpenAI(
                openai_api_key=api_key,
                model_name=model,
                openai_api_base="https://openrouter.ai/api/v1",
                temperature=0.1,
                max_tokens=1000,
            )
            logger.info(f"‚úÖ –û–±–ª–∞—á–Ω–∞—è LLM '{model}' –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
        
        # –ü–∞–º—è—Ç—å —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å–æ–æ–±—â–µ–Ω–∏–π
        self.memory = ConversationBufferWindowMemory(
            memory_key="chat_history",
            return_messages=True,
            k=6,  # –•—Ä–∞–Ω–∏–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 3 –ø–∞—Ä—ã –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç
            output_key="answer"
        )
        
        # ‚≠ê –û–ë–ù–û–í–õ–ï–ù–ù–´–ô –ü–†–û–ú–ü–¢ –° –ò–°–¢–û–†–ò–ï–ô –î–ò–ê–õ–û–ì–ê
        custom_prompt = PromptTemplate(
#             template="""–¢—ã - –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é. –û—Ç–≤–µ—á–∞–π –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –∏—Å–ø–æ–ª—å–∑—É—è –∫–æ–Ω—Ç–µ–∫—Å—Ç –∫–æ–¥–∞ –∏ –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞.

# –ò–°–¢–û–†–ò–Ø –î–ò–ê–õ–û–ì–ê:
# {chat_history}

# –ö–û–ù–¢–ï–ö–°–¢ –ö–û–î–ê:
# {context}

# –í–û–ü–†–û–°: {question}

# –ü–†–û–ê–õ–ò–ó–ò–†–£–ô –ö–û–ù–¢–ï–ö–°–¢:
# - –ï—Å–ª–∏ –µ—Å—Ç—å –∫–æ–¥ - –æ–±—ä—è—Å–Ω–∏ –µ–≥–æ —Ä–∞–±–æ—Ç—É
# - –ï—Å–ª–∏ –µ—Å—Ç—å —Ç–µ–∫—Å—Ç - –∏—Å–ø–æ–ª—å–∑—É–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –Ω–µ–≥–æ
# - –°–≤—è–∂–∏ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –∏–∑ –∫–æ–¥–∞ –∏ —Ç–µ–∫—Å—Ç–∞
# - –ü—Ä–∏–≤–µ–¥–∏ –ø—Ä–∏–º–µ—Ä—ã –µ—Å–ª–∏ –≤–æ–∑–º–æ–∂–Ω–æ

# –¢–†–ï–ë–û–í–ê–ù–ò–Ø –ö –û–¢–í–ï–¢–£:
# - –û—Ç–≤–µ—á–∞–π —á–µ—Ç–∫–æ –∏ —Ñ–∞–∫—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–∏
# - –ù–ï –∏—Å–ø–æ–ª—å–∑—É–π: "—Å—É–¥—è –ø–æ –≤—Å–µ–º—É", "–Ω–∞–≤–µ—Ä–Ω–æ–µ", "–≤–µ—Ä–æ—è—Ç–Ω–æ", "–≤–æ–∑–º–æ–∂–Ω–æ", "–∫–∞–∂–µ—Ç—Å—è", "—Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ"
# - –ù–ï –∏—Å–ø–æ–ª—å–∑—É–π –≤—ã—Ä–∞–∂–µ–Ω–∏—è –Ω–µ—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
# - –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ - —Å–∫–∞–∂–∏ —ç—Ç–æ –ø—Ä—è–º–æ
# - –û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —É—Ç–≤–µ—Ä–¥–∏—Ç–µ–ª—å–Ω—ã–º –∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º

# –ü–†–ê–í–ò–õ–ê:
# - –ò—Å–ø–æ–ª—å–∑—É–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –æ—Ç–≤–µ—Ç–∞, –Ω–æ –ù–ï —Ü–∏—Ç–∏—Ä—É–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
# - –ù–ï –ø—Ä–∏–≤–æ–¥–∏ –ø—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞ –∏–ª–∏ —Ç–µ–∫—Å—Ç–∞ –¥–æ—Å–ª–æ–≤–Ω–æ –∏–∑ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
# - –ù–ï –≥–æ–≤–æ—Ä–∏ "–≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –µ—Å—Ç—å", "–≤ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø—Ä–∏–≤–µ–¥–µ–Ω", "–∫–∞–∫ –ø–æ–∫–∞–∑–∞–Ω–æ –≤ –ø—Ä–∏–º–µ—Ä–µ"
# - –û–±–æ–±—â–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏ —Å—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π –æ—Ç–≤–µ—Ç —Å–≤–æ–∏–º–∏ —Å–ª–æ–≤–∞–º–∏
# - –û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –Ω–æ –Ω–µ —Å–æ–¥–µ—Ä–∂–∞—Ç—å –ø—Ä—è–º—ã—Ö —Ü–∏—Ç–∞—Ç

# –û–¢–í–ï–¢:""",
        template="""
–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç. 
–ò—Å–ø–æ–ª—å–∑—É–π –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç —Ç–µ–∫—Å—Ç–∞ –∫–∞–∫ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. 
–û—Ç–≤–µ—á–∞–π –ø–æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –æ–ø–∏—Ä–∞—è—Å—å –Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç.
–ï—Å–ª–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç–≤–µ—Ç–∞, —Ç–æ –æ–ø–∏—Ä–∞–π—Å—è –Ω–∞ —Å–≤–æ–∏ –≤—ã–≤–æ–¥—ã. 
–°–æ—Ö—Ä–∞–Ω—è–π –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π, –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–π —Ç–æ–Ω.

–ò–°–¢–û–†–ò–Ø –î–ò–ê–õ–û–ì–ê:
{chat_history}

–ö–û–ù–¢–ï–ö–°–¢ –ö–û–î–ê:
{context}

–í–û–ü–†–û–°: {question}

–û–¢–í–ï–¢:""",
            input_variables=["context", "question", "chat_history"]
        )
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ø–æ—á–∫–∏ —Å –¥–∏–∞–ª–æ–≥–æ–º
        self.qa_chain = ConversationalRetrievalChain.from_llm(
            llm=self.llm,
            retriever=self.vector_store.as_retriever(
                search_type="mmr",
                search_kwargs={"k": 4}
            ),
            memory=self.memory,
            return_source_documents=True,
            combine_docs_chain_kwargs={"prompt": custom_prompt},
            verbose=False,
            max_tokens_limit=None,
            condense_question_llm=self.llm,
            get_chat_history=lambda chat_history: chat_history
        )
        
        logger.info("‚úÖ ConversationRetrievalChain –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∏—Å—Ç–æ—Ä–∏–∏")

    def ask_llm(self, question: str) -> Dict[str, Any]:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –¥–ª—è –≤–æ–ø—Ä–æ—Å–æ–≤ —Å –∏—Å—Ç–æ—Ä–∏–µ–π"""
        if not self.qa_chain:
            raise ValueError("–°–Ω–∞—á–∞–ª–∞ –≤—ã–∑–æ–≤–∏—Ç–µ init_llm()")
            
        try:
            result = self.qa_chain({"question": question})
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º source_documents
            source_docs = []
            for doc in result.get("source_documents", []):
                source_docs.append({
                    "content": doc.page_content[:300] + "..." if len(doc.page_content) > 300 else doc.page_content,
                    "metadata": doc.metadata,
                    "source": doc.metadata.get("source", "unknown")
                })
            
            return {
                "result": result["answer"],
                "source_documents": source_docs,
                "conversation_history": self.get_conversation_history()
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤ ask_llm: {e}")
            return {
                "result": f"–û—à–∏–±–∫–∞: {str(e)}",
                "source_documents": [],
                "conversation_history": self.get_conversation_history()
            }

    def get_conversation_history(self) -> List[Dict[str, str]]:
        """–ü–æ–ª—É—á–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞"""
        if not self.memory:
            return []
            
        memory_vars = self.memory.load_memory_variables({})
        chat_history = memory_vars.get("chat_history", [])
        
        formatted = []
        for msg in chat_history:
            if hasattr(msg, 'type'):
                role = "user" if msg.type == "human" else "assistant"
                formatted.append({
                    "role": role,
                    "content": msg.content
                })
        
        return formatted

    def clear_conversation_history(self):
        """–û—á–∏—Å—Ç–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞ –ü–æ–∫–∞ –¥–æ–±–∞–≤–∏–ª –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ"""
        if self.memory:
            self.memory.clear()
            logger.info("–ò—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞ –æ—á–∏—â–µ–Ω–∞")

    def scan_dataset(self, project_path: str, file_extensions: List[str] = None) -> List[Document]:
        """–°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ —Ñ–∞–π–ª–æ–≤"""
        if file_extensions is None:
            # todo –¥–æ–±–∞–≤–∏—Ç—å –¥—Ä—É–≥–∏–µ —Ç–∏–ø—ã —Ñ–∞–π–ª–æ–≤ –ø–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ - –∫–∞—Ä—Ç–∏–Ω–∫–∏ –ø–æ-–ª—é–±–æ–º—É
            file_extensions = ['.py', '.md', '.txt', '.pdf', '.html']
        
        documents = []

        for root, dirs, files in os.walk(project_path):
            dirs[:] = [
                d
                for d in dirs
                if not d.startswith(".") and d not in ["__pycache__", "venv", "env"]
            ]

            for file in files:
                file_ext = os.path.splitext(file)[1].lower()
                if file_ext in file_extensions:
                    file_path = os.path.join(root, file)
                    try:
                        file_docs = FileProcessorFacade.parse_file(file_path, Document)
                        documents.extend(file_docs)
                        print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω: {file_path} ({len(file_docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)")
                    except Exception as e:
                        print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {file_path}: {e}")

        return documents

    def add_documents_to_vectorstore(self, documents: List[Document]):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –ë–î"""
        if not documents:
            raise ValueError("–ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")

        chunks = self.code_splitter.split_documents(documents)
        print(f"üìÑ –°–æ–∑–¥–∞–Ω–æ {len(chunks)} –∫–æ–¥-—á–∞–Ω–∫–æ–≤")

        self.vector_store.add_documents(chunks)
        print("‚úÖ –ö–æ–¥ –¥–æ–±–∞–≤–ª–µ–Ω –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –ë–î")
