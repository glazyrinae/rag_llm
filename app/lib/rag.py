# app/rag_system.py
import os
from typing import List, Dict, Any
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Qdrant
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate
from qdrant_client import QdrantClient
from lib.file_processor import FileProcessorFacade


class Rag:
    def __init__(
        self,
        embeddings: str = "sentence-transformers/all-mpnet-base-v2",
        cache_dir: str = "/app/embedding_models",
    ):
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –∫—ç—à–∞, –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        os.makedirs(cache_dir, exist_ok=True)

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Qdrant –∫–ª–∏–µ–Ω—Ç–∞
        self.qdrant_client = QdrantClient(
            host=os.getenv("QDRANT_HOST", "localhost"),
            port=int(os.getenv("QDRANT_PORT", "6333")),
        )

        # –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –∫–æ–¥–∞ —Å –ª–æ–∫–∞–ª—å–Ω—ã–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º
        self.embeddings = HuggingFaceEmbeddings(
            model_name=embeddings,
            cache_folder=cache_dir,  # –õ–æ–∫–∞–ª—å–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –º–æ–¥–µ–ª–∏ –µ—Å–ª–∏ –º–æ–¥–µ–ª—å —Ç–∞–º –ª–µ–∂–∏—Ç –æ–Ω –Ω–µ –±—É–¥–µ—Ç —Å–∫–∞—á–∏–≤–∞—Ç—å –ø–æ–≤—Ç–æ—Ä–Ω–æ
            model_kwargs={"device": "cpu"},  # –∏–ª–∏ 'cuda' –µ—Å–ª–∏ –µ—Å—Ç—å GPU
        )

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –º–æ–¥–µ–ª—å –ª–æ–∫–∞–ª—å–Ω–æ, –µ—Å–ª–∏ –Ω–µ—Ç - —Å–∫–∞—á–∏–≤–∞–µ–º
        # self._ensure_embeddings_available(embeddings, cache_dir)

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î
        self.vector_store = Qdrant(
            client=self.qdrant_client,
            collection_name="python_code",
            embeddings=self.embeddings,
        )

        # –°–ø–ª–∏—Ç—Ç–µ—Ä –¥–ª—è Python –∫–æ–¥–∞
        self.code_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=150,
            separators=[
                "\n\nclass ",
                "\n\ndef ",
                "\n\nasync def ",
                "\n\n# ",
                "\n\n",
                "\n",
                " ",
            ],
            length_function=len,
        )

        self.llm = None

    # –û—Å—Ç–∞–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã –æ—Å—Ç–∞—é—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π...
    def init_llm(self, api_key: str, model: str = "mistralai/mistral-7b-instruct"):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ–±–ª–∞—á–Ω–æ–π LLM"""
        self.llm = ChatOpenAI(
            openai_api_key=api_key,
            model_name=model,
            openai_api_base="https://openrouter.ai/api/v1",
            temperature=0.1,
            max_tokens=1000,
        )

    def scan_dataset(self, project_path: str, file_extensions: List[str] = None) -> List[Document]:
        """–°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ —Ñ–∞–π–ª–æ–≤"""
        if file_extensions is None:
            file_extensions = ['.py', '.md', '.txt', '.pdf', '.html']
        
        documents = []

        for root, dirs, files in os.walk(project_path):
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
            dirs[:] = [
                d
                for d in dirs
                if not d.startswith(".") and d not in ["__pycache__", "venv", "env"]
            ]

            for file in files:
                file_ext = os.path.splitext(file)[1].lower()
                if file_ext in file_extensions:
                    file_path = os.path.join(root, file)
                    try:
                        # –§–∞—Å–∞–¥ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±–µ—Ä–µ—Ç –Ω—É–∂–Ω—ã–π –ø–∞—Ä—Å–µ—Ä
                        file_docs = FileProcessorFacade.parse_file(file_path, Document)
                        documents.extend(file_docs)
                        print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω: {file_path} ({len(file_docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)")
                    except Exception as e:
                        print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {file_path}: {e}")

        return documents

    def add_documents_to_vectorstore(self, documents: List[Document]):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –ë–î"""
        if not documents:
            raise ValueError("–ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")

        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏
        chunks = self.code_splitter.split_documents(documents)
        print(f"üìÑ –°–æ–∑–¥–∞–Ω–æ {len(chunks)} –∫–æ–¥-—á–∞–Ω–∫–æ–≤")

        # –î–æ–±–∞–≤–ª—è–µ–º –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –ë–î
        self.vector_store.add_documents(chunks)
        print("‚úÖ –ö–æ–¥ –¥–æ–±–∞–≤–ª–µ–Ω –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –ë–î")

    def create_code_qa_chain(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ QA —Ü–µ–ø–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –∫–æ–¥–æ–º"""
        if not self.llm:
            raise ValueError("LLM –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")

        # –ü—Ä–æ–º–ø—Ç –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞
        prompt_template = """–¢—ã - –æ–ø—ã—Ç–Ω—ã–π Python —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫. –ò—Å–ø–æ–ª—å–∑—É–π –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –∫–æ–¥ –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å.

–ö–û–ù–¢–ï–ö–°–¢ –ö–û–î–ê:
{context}

–í–û–ü–†–û–°: {question}

–û—Ç–≤–µ—Ç—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø–æ–¥—Ä–æ–±–Ω–æ –∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏ –≥—Ä–∞–º–æ—Ç–Ω–æ. –ï—Å–ª–∏ –≤ –∫–æ–¥–µ –µ—Å—Ç—å –ø—Ä–∏–º–µ—Ä—ã - –∏—Å–ø–æ–ª—å–∑—É–π –∏—Ö. –ï—Å–ª–∏ –Ω—É–∂–Ω–æ –æ–±—ä—è—Å–Ω–∏—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É - —Å–¥–µ–ª–∞–π —ç—Ç–æ.

–û–¢–í–ï–¢:"""

        prompt = PromptTemplate(
            template=prompt_template, input_variables=["context", "question"]
        )

        return RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=self.vector_store.as_retriever(
                search_type="similarity", search_kwargs={"k": 6}
            ),
            chain_type_kwargs={"prompt": prompt},
            return_source_documents=True,
        )

    def ask_about_code(self, question: str) -> Dict[str, Any]:
        """–ó–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å –æ –∫–æ–¥–µ"""
        qa_chain = self.create_code_qa_chain()
        return qa_chain({"query": question})
