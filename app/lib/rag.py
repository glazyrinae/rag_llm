# app/rag_system.py
import os
from typing import List, Dict, Any
import logging
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferWindowMemory
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Qdrant
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate
from qdrant_client import QdrantClient
from lib.file_processor import FileProcessorFacade

logger = logging.getLogger(__name__)

class Rag:
    def __init__(
        self,
        embeddings: str = "sentence-transformers/all-mpnet-base-v2",
        cache_dir: str = "/app/embedding_models",
    ):
        os.makedirs(cache_dir, exist_ok=True)

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Qdrant –∫–ª–∏–µ–Ω—Ç–∞
        self.qdrant_client = QdrantClient(
            host=os.getenv("QDRANT_HOST", "localhost"),
            port=int(os.getenv("QDRANT_PORT", "6333")),
        )

        # –≠–º–±–µ–¥–¥–∏–Ω–≥–∏
        self.embeddings = HuggingFaceEmbeddings(
            model_name=embeddings,
            cache_folder=cache_dir,
            model_kwargs={"device": "cpu"},
        )

        # –í–µ–∫—Ç–æ—Ä–Ω–∞—è –ë–î
        self.vector_store = Qdrant(
            client=self.qdrant_client,
            collection_name="python_code",
            embeddings=self.embeddings
        )

        # –°–ø–ª–∏—Ç—Ç–µ—Ä –¥–ª—è –∫–æ–¥–∞
        self.code_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=150,
            separators=["\n\nclass ", "\n\ndef ", "\n\nasync def ", "\n\n# ", "\n\n", "\n", " "],
            length_function=len,
        )

        self.llm = None
        self.qa_chain = None
        self.memory = None

    def init_llm(self, api_key: str, model: str = "Qwen/Qwen3-Coder-30B-A3B-Instruct"):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ConversationRetrievalChain"""
        self.llm = ChatOpenAI(
            openai_api_key=api_key,
            model_name=model,
            openai_api_base="https://openrouter.ai/api/v1",
            temperature=0.1,
            max_tokens=1000,
        )
        
        # –ü–∞–º—è—Ç—å —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å–æ–æ–±—â–µ–Ω–∏–π
        self.memory = ConversationBufferWindowMemory(
            memory_key="chat_history",
            return_messages=True,
            k=6,  # –•—Ä–∞–Ω–∏–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 3 –ø–∞—Ä—ã –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç
            output_key="answer"
        )
        
        # ‚≠ê –û–ë–ù–û–í–õ–ï–ù–ù–´–ô –ü–†–û–ú–ü–¢ –° –ò–°–¢–û–†–ò–ï–ô –î–ò–ê–õ–û–ì–ê
        custom_prompt = PromptTemplate(
            template="""–¢—ã - –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é. –û—Ç–≤–µ—á–∞–π –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –∏—Å–ø–æ–ª—å–∑—É—è –∫–æ–Ω—Ç–µ–∫—Å—Ç –∫–æ–¥–∞ –∏ –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞.

–ò–°–¢–û–†–ò–Ø –î–ò–ê–õ–û–ì–ê:
{chat_history}

–ö–û–ù–¢–ï–ö–°–¢ –ö–û–î–ê:
{context}

–í–û–ü–†–û–°: {question}

–ü–†–û–ê–õ–ò–ó–ò–†–£–ô –ö–û–ù–¢–ï–ö–°–¢:
- –ï—Å–ª–∏ –µ—Å—Ç—å –∫–æ–¥ - –æ–±—ä—è—Å–Ω–∏ –µ–≥–æ —Ä–∞–±–æ—Ç—É
- –ï—Å–ª–∏ –µ—Å—Ç—å —Ç–µ–∫—Å—Ç - –∏—Å–ø–æ–ª—å–∑—É–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –Ω–µ–≥–æ
- –°–≤—è–∂–∏ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –∏–∑ –∫–æ–¥–∞ –∏ —Ç–µ–∫—Å—Ç–∞
- –ü—Ä–∏–≤–µ–¥–∏ –ø—Ä–∏–º–µ—Ä—ã –µ—Å–ª–∏ –≤–æ–∑–º–æ–∂–Ω–æ

–¢–†–ï–ë–û–í–ê–ù–ò–Ø –ö –û–¢–í–ï–¢–£:
- –û—Ç–≤–µ—á–∞–π —á–µ—Ç–∫–æ –∏ —Ñ–∞–∫—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–∏
- –ù–ï –∏—Å–ø–æ–ª—å–∑—É–π: "—Å—É–¥—è –ø–æ –≤—Å–µ–º—É", "–Ω–∞–≤–µ—Ä–Ω–æ–µ", "–≤–µ—Ä–æ—è—Ç–Ω–æ", "–≤–æ–∑–º–æ–∂–Ω–æ", "–∫–∞–∂–µ—Ç—Å—è", "—Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ"
- –ù–ï –∏—Å–ø–æ–ª—å–∑—É–π –≤—ã—Ä–∞–∂–µ–Ω–∏—è –Ω–µ—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
- –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ - —Å–∫–∞–∂–∏ —ç—Ç–æ –ø—Ä—è–º–æ
- –û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —É—Ç–≤–µ—Ä–¥–∏—Ç–µ–ª—å–Ω—ã–º –∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º

–ü–†–ê–í–ò–õ–ê:
- –ò—Å–ø–æ–ª—å–∑—É–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –æ—Ç–≤–µ—Ç–∞, –Ω–æ –ù–ï —Ü–∏—Ç–∏—Ä—É–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
- –ù–ï –ø—Ä–∏–≤–æ–¥–∏ –ø—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞ –∏–ª–∏ —Ç–µ–∫—Å—Ç–∞ –¥–æ—Å–ª–æ–≤–Ω–æ –∏–∑ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
- –ù–ï –≥–æ–≤–æ—Ä–∏ "–≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –µ—Å—Ç—å", "–≤ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø—Ä–∏–≤–µ–¥–µ–Ω", "–∫–∞–∫ –ø–æ–∫–∞–∑–∞–Ω–æ –≤ –ø—Ä–∏–º–µ—Ä–µ"
- –û–±–æ–±—â–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏ —Å—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π –æ—Ç–≤–µ—Ç —Å–≤–æ–∏–º–∏ —Å–ª–æ–≤–∞–º–∏
- –û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –Ω–æ –Ω–µ —Å–æ–¥–µ—Ä–∂–∞—Ç—å –ø—Ä—è–º—ã—Ö —Ü–∏—Ç–∞—Ç

–û–¢–í–ï–¢:""",
            input_variables=["context", "question", "chat_history"]
        )
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ø–æ—á–∫–∏ —Å –¥–∏–∞–ª–æ–≥–æ–º
        self.qa_chain = ConversationalRetrievalChain.from_llm(
            llm=self.llm,
            retriever=self.vector_store.as_retriever(
                search_type="similarity",
                search_kwargs={"k": 4}
            ),
            memory=self.memory,
            return_source_documents=True,
            combine_docs_chain_kwargs={"prompt": custom_prompt},
            verbose=False,
            max_tokens_limit=None,
            condense_question_llm=self.llm,
            get_chat_history=lambda chat_history: chat_history
        )
        
        logger.info("‚úÖ ConversationRetrievalChain –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∏—Å—Ç–æ—Ä–∏–∏")

    def ask_llm(self, question: str) -> Dict[str, Any]:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –¥–ª—è –≤–æ–ø—Ä–æ—Å–æ–≤ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∏—Å—Ç–æ—Ä–∏–µ–π"""
        if not self.qa_chain:
            raise ValueError("–°–Ω–∞—á–∞–ª–∞ –≤—ã–∑–æ–≤–∏—Ç–µ init_llm()")
            
        try:
            result = self.qa_chain({"question": question})
            
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º source_documents
            source_docs = []
            for doc in result.get("source_documents", []):
                source_docs.append({
                    "content": doc.page_content[:300] + "..." if len(doc.page_content) > 300 else doc.page_content,
                    "metadata": doc.metadata,
                    "source": doc.metadata.get("source", "unknown")
                })
            
            return {
                "result": result["answer"],
                "source_documents": source_docs,
                "conversation_history": self.get_conversation_history()
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤ ask_llm: {e}")
            return {
                "result": f"–û—à–∏–±–∫–∞: {str(e)}",
                "source_documents": [],
                "conversation_history": self.get_conversation_history()
            }

    def get_conversation_history(self) -> List[Dict[str, str]]:
        """–ü–æ–ª—É—á–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞"""
        if not self.memory:
            return []
            
        memory_vars = self.memory.load_memory_variables({})
        chat_history = memory_vars.get("chat_history", [])
        
        formatted = []
        for msg in chat_history:
            if hasattr(msg, 'type'):
                role = "user" if msg.type == "human" else "assistant"
                formatted.append({
                    "role": role,
                    "content": msg.content
                })
        
        return formatted

    def clear_conversation_history(self):
        """–û—á–∏—Å—Ç–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞ –ü–æ–∫–∞ –¥–æ–±–∞–≤–∏–ª –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ"""
        if self.memory:
            self.memory.clear()
            logger.info("–ò—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞ –æ—á–∏—â–µ–Ω–∞")

    # –û—Å—Ç–∞–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π
    def scan_dataset(self, project_path: str, file_extensions: List[str] = None) -> List[Document]:
        """–°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ —Ñ–∞–π–ª–æ–≤"""
        if file_extensions is None:
            file_extensions = ['.py', '.md', '.txt', '.pdf', '.html']
        
        documents = []

        for root, dirs, files in os.walk(project_path):
            dirs[:] = [
                d
                for d in dirs
                if not d.startswith(".") and d not in ["__pycache__", "venv", "env"]
            ]

            for file in files:
                file_ext = os.path.splitext(file)[1].lower()
                if file_ext in file_extensions:
                    file_path = os.path.join(root, file)
                    try:
                        file_docs = FileProcessorFacade.parse_file(file_path, Document)
                        documents.extend(file_docs)
                        print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω: {file_path} ({len(file_docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)")
                    except Exception as e:
                        print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {file_path}: {e}")

        return documents

    def add_documents_to_vectorstore(self, documents: List[Document]):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –ë–î"""
        if not documents:
            raise ValueError("–ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")

        chunks = self.code_splitter.split_documents(documents)
        print(f"üìÑ –°–æ–∑–¥–∞–Ω–æ {len(chunks)} –∫–æ–¥-—á–∞–Ω–∫–æ–≤")

        self.vector_store.add_documents(chunks)
        print("‚úÖ –ö–æ–¥ –¥–æ–±–∞–≤–ª–µ–Ω –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –ë–î")
