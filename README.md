# RAG LLM: Система поиска и анализа кода с помощью Retrieval-Augmented Generation

---

## Оглавление

- [Описание](#описание)
- [Возможности](#возможности)
- [Архитектура и компоненты](#архитектура-и-компоненты)
- [Структура проекта](#структура-проекта)
- [Установка и запуск](#установка-и-запуск)
  - [Требования](#требования)
  - [Переменные окружения](#переменные-окружения)
  - [Запуск через Docker Compose](#запуск-через-docker-compose)
  - [Локальный запуск (без Docker)](#локальный-запуск-без-docker)
- [Использование](#использование)
  - [Индексация кода](#индексация-кода)
  - [Задание вопросов](#задание-вопросов)
  - [Асинхронная очередь](#асинхронная-очередь)
  - [Примеры запросов](#примеры-запросов)
- [Описание основных файлов и модулей](#описание-основных-файлов-и-модулей)
- [Расширение и кастомизация](#расширение-и-кастомизация)
- [Лицензия](#лицензия)
- [Авторы и благодарности](#авторы-и-благодарности)
- [FAQ](#faq)

---

## Описание

**RAG LLM** — это система для интеллектуального поиска, анализа и генерации ответов по исходному коду Python-проектов с использованием Retrieval-Augmented Generation (RAG), векторных баз данных и современных языковых моделей (LLM). Проект предназначен для автоматизации технической документации, аудита кода, поддержки разработчиков и ускорения онбординга в проекты.

---

## Возможности

- **Индексация исходного кода:**  
  Автоматическое сканирование директорий с исходным кодом, разбиение на логические блоки (файлы, классы, функции, docstring-и), извлечение метаданных и сохранение эмбеддингов в векторную базу данных (Qdrant).

- **Семантический поиск:**  
  Быстрый поиск по коду на основе смыслового сходства, а не только по ключевым словам.

- **Генерация ответов с помощью LLM:**  
  Ответы на вопросы о коде с использованием современных языковых моделей (например, Mistral через OpenRouter API), с учетом найденных релевантных фрагментов кода.

- **Асинхронная очередь задач:**  
  Поддержка асинхронной обработки запросов через очередь (Redis), что позволяет масштабировать систему и обрабатывать множество запросов параллельно.

- **REST API:**  
  Удобный интерфейс для интеграции с другими сервисами и фронтендом.

- **Логирование и мониторинг:**  
  Подробное логирование всех этапов обработки запросов, возможность отслеживать статус задач.

- **Масштабируемость:**  
  Возможность развертывания в Docker-контейнерах, поддержка горизонтального масштабирования.

---

## Архитектура и компоненты

```
+-------------------+      +-------------------+      +-------------------+
|                   |      |                   |      |                   |
|   FastAPI (API)   +----->+   Redis (Queue)   +----->+   Worker (RAG)    |
|                   |      |                   |      |                   |
+-------------------+      +-------------------+      +-------------------+
         |                          |                          |
         v                          v                          v
+-------------------+      +-------------------+      +-------------------+
|                   |      |                   |      |                   |
|  Qdrant (Vectors) |      |  Embedding Model  |      |   LLM (OpenRouter)|
|                   |      |                   |      |                   |
+-------------------+      +-------------------+      +-------------------+
```

- **FastAPI** — REST API для взаимодействия с пользователем.
- **Redis** — брокер очереди для асинхронных задач.
- **Worker** — обработчик задач очереди, реализующий логику RAG.
- **Qdrant** — векторная база данных для хранения эмбеддингов кода.
- **Embedding Model** — модель для преобразования текста кода в векторное представление (по умолчанию `sentence-transformers/all-mpnet-base-v2`).
- **LLM** — языковая модель для генерации ответов (например, Mistral через OpenRouter).

---

## Структура проекта

```
rag_llm/
├── app/
│   ├── main.py                # Точка входа FastAPI
│   ├── manage.py              # Запуск воркера очереди
│   ├── api/
│   │   ├── endpoints.py       # REST API endpoints
│   │   └── ...                # Доп. модули API
│   ├── lib/
│   │   ├── rag.py             # Класс Rag: индексация, поиск, генерация
│   │   ├── queue.py           # Класс QueueManager: очередь задач
│   │   ├── file_utils.py      # Утилиты для работы с файлами
│   │   └── ...                # Прочие вспомогательные модули
│   ├── services/
│   │   ├── rag_adapter.py     # DI и адаптеры для RAG
│   │   └── ...                # Прочие сервисы
│   └── data/                  # (опционально) исходные проекты для индексации
├── deploy/
│   ├── Dockerfile
│   ├── requirements.txt
│   └── embedding_models/      # Локальные модели эмбеддингов
├── docker-compose.yml
├── .env
└── README.md
```

---

## Установка и запуск

### Требования

- **Docker** и **Docker Compose** (рекомендуется для быстрого старта)
- Для локального запуска: Python 3.10+, pip, git
- Доступ к интернету для скачивания моделей и обращения к OpenRouter

### Переменные окружения

Создайте файл `.env` в корне проекта. Пример содержимого:

```
OPENROUTER_API_KEY=your_openrouter_api_key
QDRANT_HOST=qdrant
QDRANT_PORT=6333
REDIS_URL=redis://redis
EMBEDDING_MODEL=sentence-transformers/all-mpnet-base-v2
```

### Запуск через Docker Compose

1. Клонируйте репозиторий:

    ```sh
    git clone <repo_url>
    cd rag_llm
    ```

2. Скопируйте пример переменных окружения:

    ```sh
    cp .env.example .env
    ```

3. Запустите сервисы:

    ```sh
    docker-compose up --build
    ```

    Будут подняты:
    - Qdrant (векторная БД)
    - Redis (брокер очереди)
    - FastAPI (REST API)
    - Worker (обработчик очереди)

4. API будет доступен по адресу:  
   http://localhost:5005/docs (Swagger UI)

### Локальный запуск (без Docker)

1. Установите зависимости:

    ```sh
    python3 -m venv venv
    source venv/bin/activate
    pip install -r deploy/requirements.txt
    ```

2. Запустите Qdrant и Redis (например, через Docker):

    ```sh
    docker run -p 6333:6333 -p 6334:6334 qdrant/qdrant
    docker run -p 6379:6379 redis
    ```

3. Запустите FastAPI:

    ```sh
    uvicorn app.main:app --host 0.0.0.0 --port 5005
    ```

4. Запустите воркер очереди:

    ```sh
    python app/manage.py
    ```

---

## Использование

### Индексация кода

Для индексации локального проекта (например, в `/app/data/your_project`):

```sh
curl -X POST "http://localhost:5005/api/index-local-project" \
     -H "accept: application/json" \
     -d "project_path=/app/data/your_project"
```

- **project_path** — путь к директории с исходным кодом внутри контейнера.

После индексации все функции, классы и файлы будут разбиты на фрагменты, преобразованы в эмбеддинги и сохранены в Qdrant.

### Задание вопросов

**Синхронный режим (без очереди):**

```sh
curl -X POST "http://localhost:5005/api/ask_test" \
     -H "accept: application/json" \
     -d "question=Что делает функция X?"
```

**Асинхронный режим (через очередь):**

```sh
curl -X POST "http://localhost:5005/api/ask" \
     -H "accept: application/json" \
     -d "question=Как работает класс Y?"
```

- В асинхронном режиме вы получите идентификатор задачи (`task_id`), по которому можно проверить статус и получить результат.

### Асинхронная очередь

Проверить статус задачи:

```sh
curl "http://localhost:5005/api/task-status?task_id=<task_id>"
```

Получить результат:

```sh
curl "http://localhost:5005/api/task-result?task_id=<task_id>"
```

### Примеры запросов

#### Проверка статуса API

```sh
curl http://localhost:5005/api/health
```

#### Индексация кода

```sh
curl -X POST "http://localhost:5005/api/index-local-project" \
     -d "project_path=/app/data/your_project"
```

#### Вопрос по коду

```sh
curl -X POST "http://localhost:5005/api/ask_test" \
     -d "question=Как работает функция foo?"
```

#### Асинхронный вопрос

```sh
curl -X POST "http://localhost:5005/api/ask" \
     -d "question=Опиши архитектуру проекта"
```

---

## Описание основных файлов и модулей

- **app/main.py** — точка входа FastAPI, определяет маршруты и инициализацию приложения.
- **app/manage.py** — запуск воркера очереди, обработка асинхронных задач.
- **app/api/endpoints.py** — основные REST endpoints: индексация, вопросы, статус задач.
- **app/lib/rag.py** — класс `Rag`, реализующий:
  - Индексацию кода (сканирование, разбиение, эмбеддинг, сохранение в Qdrant)
  - Поиск релевантных фрагментов по запросу
  - Формирование prompt-а для LLM
  - Вызов LLM и генерацию ответа
- **app/lib/queue.py** — класс `QueueManager`, реализующий:
  - Постановку задач в очередь Redis
  - Получение статуса и результата задач
- **app/services/rag_adapter.py** — адаптер для вызова RAG из очереди.
- **deploy/Dockerfile** — сборка образа приложения.
- **deploy/requirements.txt** — зависимости Python.
- **docker-compose.yml** — описание сервисов для запуска всей системы.

---

## Расширение и кастомизация

- **Добавление новых языков программирования:**  
  Реализуйте парсеры для других языков (например, Java, JS) и добавьте их в модуль индексации.

- **Замена модели эмбеддингов:**  
  Измените переменную окружения `EMBEDDING_MODEL` и скачайте нужную модель в `deploy/embedding_models`.

- **Использование других LLM:**  
  Поддерживаются любые LLM, доступные через OpenRouter или напрямую через API.

- **Интеграция с фронтендом:**  
  Используйте REST API для создания пользовательских интерфейсов (например, чат-бота для документации).

- **Масштабирование:**  
  Запускайте несколько воркеров очереди для обработки большого количества запросов.

---

## Лицензия

Проект распространяется под лицензией Apache-2.0.

---

## Авторы и благодарности

- Используются модели [sentence-transformers/all-mpnet-base-v2](https://www.sbert.net/), инфраструктура [Qdrant](https://qdrant.tech/), [Redis](https://redis.io/).
- Благодарности HuggingFace, OpenRouter, LangChain и сообществу Python.
- Автор: [Ваше имя или команда]

---

## FAQ

**Q:** Как добавить новый проект для индексации?  
**A:** Поместите проект в папку `app/data/` (или другую, указанную в `project_path`) и вызовите endpoint `/api/index-local-project`.

**Q:** Как ускорить индексацию?  
**A:** Используйте локальные модели эмбеддингов, увеличьте количество воркеров очереди.

**Q:** Как изменить модель LLM?  
**A:** Измените настройки в `.env` и перезапустите сервисы.

**Q:** Как получить подробный лог?  
**A:** Логи пишутся в stdout контейнеров, используйте `docker-compose logs` для просмотра.

**Q:** Как интегрировать с CI/CD?  
**A:** Используйте REST API для автоматической индексации и анализа кода при каждом коммите.

---

**Если у вас есть вопросы или предложения — создайте issue или pull request!**